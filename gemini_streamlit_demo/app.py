import streamlit as st
import google.generativeai as genai
from datetime import datetime
import os
from dotenv import load_dotenv

# Táº£i biáº¿n mÃ´i trÆ°á»ng tá»« file .env náº¿u cÃ³
load_dotenv()

# Cáº¥u hÃ¬nh trang Streamlit
st.set_page_config(
    page_title="Gemini Chat Demo vá»›i RAG",
    page_icon="ğŸ’¬",
    layout="wide"
)

# API key cá»‘ Ä‘á»‹nh
GEMINI_API_KEY = "AIzaSyA0BgHLDCU9yoiv7JAbCoUmJrrtzkkWoV4"

# HÃ m Ä‘á»c file data.markdown
def read_markdown_file(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()
    except Exception as e:
        return f"Lá»—i khi Ä‘á»c file: {e}"

# HÃ m khá»Ÿi táº¡o session state
def init_session_state():
    # Sá»­ dá»¥ng API key cá»‘ Ä‘á»‹nh
    if "api_key" not in st.session_state:
        st.session_state.api_key = GEMINI_API_KEY
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "gemini_model" not in st.session_state:
        st.session_state.gemini_model = "gemini-2.0-flash"
    if "temperature" not in st.session_state:
        st.session_state.temperature = 0.7
    if "max_output_tokens" not in st.session_state:
        st.session_state.max_output_tokens = 2048
    if "top_k" not in st.session_state:
        st.session_state.top_k = 40
    if "top_p" not in st.session_state:
        st.session_state.top_p = 0.95
    if "system_prompt" not in st.session_state:
        st.session_state.system_prompt = "Báº¡n lÃ  trá»£ lÃ½ AI há»¯u Ã­ch, lá»‹ch sá»± vÃ  trung thá»±c."
    if "full_system_prompt" not in st.session_state:
        st.session_state.full_system_prompt = f"[SYSTEM]: {st.session_state.system_prompt}"
    if "model_configured" not in st.session_state:
        st.session_state.model_configured = False
    if "data_markdown_content" not in st.session_state:
        st.session_state.data_markdown_content = ""
    if "use_rag" not in st.session_state:
        st.session_state.use_rag = True
    if "markdown_file_path" not in st.session_state:
        st.session_state.markdown_file_path = "data.markdown"
    if "active_tab" not in st.session_state:
        st.session_state.active_tab = "Chat"
    if "show_markdown_preview" not in st.session_state:
        st.session_state.show_markdown_preview = False

# Khá»Ÿi táº¡o session state
init_session_state()

# HÃ m khá»Ÿi táº¡o model Gemini
def configure_gemini_model():
    try:
        genai.configure(api_key=st.session_state.api_key)
        
        generation_config = {
            "temperature": st.session_state.temperature,
            "max_output_tokens": st.session_state.max_output_tokens,
            "top_k": st.session_state.top_k,
            "top_p": st.session_state.top_p,
        }
        
        # Táº¡o system prompt vÃ  lÆ°u vÃ o session state Ä‘á»ƒ sá»­ dá»¥ng khi gá»­i tin nháº¯n
        base_system_prompt = st.session_state.system_prompt
        
        if st.session_state.use_rag and st.session_state.data_markdown_content:
            # Náº¿u sá»­ dá»¥ng RAG, thÃªm thÃ´ng tin tá»« file markdown vÃ o system prompt
            full_system_prompt = f"""[SYSTEM]: {base_system_prompt}

Báº¡n cÃ³ quyá»n truy cáº­p vÃ o thÃ´ng tin sau:

{st.session_state.data_markdown_content}

Khi ngÆ°á»i dÃ¹ng há»i vá» ná»™i dung liÃªn quan Ä‘áº¿n thÃ´ng tin trÃªn, hÃ£y sá»­ dá»¥ng thÃ´ng tin nÃ y Ä‘á»ƒ tráº£ lá»i. 
Náº¿u cÃ¢u há»i khÃ´ng liÃªn quan Ä‘áº¿n thÃ´ng tin nÃ y, hÃ£y tráº£ lá»i dá»±a trÃªn kiáº¿n thá»©c chung cá»§a báº¡n."""
        else:
            # Náº¿u khÃ´ng sá»­ dá»¥ng RAG, sá»­ dá»¥ng system prompt gá»‘c
            full_system_prompt = f"[SYSTEM]: {base_system_prompt}"
        
        # LÆ°u system prompt vÃ o session state
        st.session_state.full_system_prompt = full_system_prompt
        
        model = genai.GenerativeModel(
            model_name=st.session_state.gemini_model,
            generation_config=generation_config
        )
        
        # Táº¡o chat session nhÆ°ng khÃ´ng sá»­ dá»¥ng system_instruction
        chat = model.start_chat(history=[])
        
        st.session_state.model = model
        st.session_state.chat = chat
        st.session_state.model_configured = True
        return True
    except Exception as e:
        st.error(f"Lá»—i khi cáº¥u hÃ¬nh model Gemini: {e}")
        st.session_state.model_configured = False
        return False

# HÃ m gá»­i tin nháº¯n vÃ  nháº­n pháº£n há»“i, tÃ­ch há»£p system prompt
def send_message(user_message):
    if not st.session_state.model_configured:
        st.error("Vui lÃ²ng cáº¥u hÃ¬nh API key vÃ  cÃ¡c thÃ´ng sá»‘ trÆ°á»›c khi gá»­i tin nháº¯n.")
        return
    
    # ThÃªm tin nháº¯n ngÆ°á»i dÃ¹ng vÃ o lá»‹ch sá»­
    timestamp = datetime.now().strftime("%H:%M:%S")
    st.session_state.messages.append({"role": "user", "content": user_message, "timestamp": timestamp})
    
    try:
        # Gá»­i tin nháº¯n káº¿t há»£p system prompt vÃ  user message
        combined_message = f"{st.session_state.full_system_prompt}\n\n[USER]: {user_message}"
        response = st.session_state.chat.send_message(combined_message)
            
        response_text = response.text
        
        # ThÃªm pháº£n há»“i vÃ o lá»‹ch sá»­
        timestamp = datetime.now().strftime("%H:%M:%S")
        st.session_state.messages.append({"role": "model", "content": response_text, "timestamp": timestamp})
    except Exception as e:
        st.error(f"Lá»—i khi gá»i API Gemini: {e}")

# HÃ m táº£i file markdown
def load_markdown_file():
    try:
        file_path = st.session_state.markdown_file_path
        content = read_markdown_file(file_path)
        st.session_state.data_markdown_content = content
        return True, f"ÄÃ£ táº£i file {file_path} thÃ nh cÃ´ng."
    except Exception as e:
        return False, f"Lá»—i khi táº£i file: {e}"

# HÃ m hiá»ƒn thá»‹ tab CÃ i Ä‘áº·t
def display_settings_tab():
    # Hiá»ƒn thá»‹ thÃ´ng bÃ¡o API key Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh sáºµn
    st.success(f"API Key Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh tá»± Ä‘á»™ng")
    
    # TÄƒng khÃ´ng gian cho System Prompt báº±ng cÃ¡ch sá»­ dá»¥ng full width
    st.subheader("System Prompt")
    system_prompt = st.text_area("System Prompt:", value=st.session_state.system_prompt, height=300)
    if system_prompt != st.session_state.system_prompt:
        st.session_state.system_prompt = system_prompt
        st.session_state.model_configured = False
    st.info("LÆ°u Ã½: Thay Ä‘á»•i System Prompt chá»‰ cÃ³ hiá»‡u lá»±c sau khi nháº¥n 'Ãp dá»¥ng cÃ i Ä‘áº·t'.")
    
    # Táº¡o 2 cá»™t cho cÃ i Ä‘áº·t model
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("CÃ i Ä‘áº·t Model")
        model_options = ["gemini-2.5-flash-preview-04-17", "gemini-2.5-pro-preview-05-06", "gemini-2.0-flash"]
        selected_model = st.selectbox("Model:", model_options, index=model_options.index(st.session_state.gemini_model) if st.session_state.gemini_model in model_options else 0)
        if selected_model != st.session_state.gemini_model:
            st.session_state.gemini_model = selected_model
            st.session_state.model_configured = False
    
    with col2:
        with st.expander("CÃ i Ä‘áº·t nÃ¢ng cao", expanded=True):
            temperature = st.slider("Temperature:", min_value=0.0, max_value=1.0, value=st.session_state.temperature, step=0.05)
            if temperature != st.session_state.temperature:
                st.session_state.temperature = temperature
                st.session_state.model_configured = False
            
            max_tokens = st.slider("Max Output Tokens:", min_value=100, max_value=8192, value=st.session_state.max_output_tokens, step=100)
            if max_tokens != st.session_state.max_output_tokens:
                st.session_state.max_output_tokens = max_tokens
                st.session_state.model_configured = False
                
            top_k = st.slider("Top K:", min_value=1, max_value=100, value=st.session_state.top_k, step=1)
            if top_k != st.session_state.top_k:
                st.session_state.top_k = top_k
                st.session_state.model_configured = False
                
            top_p = st.slider("Top P:", min_value=0.0, max_value=1.0, value=st.session_state.top_p, step=0.05)
            if top_p != st.session_state.top_p:
                st.session_state.top_p = top_p
                st.session_state.model_configured = False
    
    # NÃºt cáº¥u hÃ¬nh model
    if st.button("Ãp dá»¥ng cÃ i Ä‘áº·t", type="primary"):
        with st.spinner("Äang cáº¥u hÃ¬nh model..."):
            if configure_gemini_model():
                st.success("Cáº¥u hÃ¬nh model thÃ nh cÃ´ng!")

# HÃ m hiá»ƒn thá»‹ tab RAG
def display_rag_tab():
    st.subheader("CÃ i Ä‘áº·t RAG")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        use_rag = st.checkbox("Sá»­ dá»¥ng RAG", value=st.session_state.use_rag)
        if use_rag != st.session_state.use_rag:
            st.session_state.use_rag = use_rag
            st.session_state.model_configured = False
        
        markdown_file_path = st.text_input("ÄÆ°á»ng dáº«n file markdown:", value=st.session_state.markdown_file_path)
        if markdown_file_path != st.session_state.markdown_file_path:
            st.session_state.markdown_file_path = markdown_file_path
        
        if st.button("Táº£i file markdown", type="primary"):
            success, message = load_markdown_file()
            if success:
                st.success(message)
                # Reset model_configured Ä‘á»ƒ buá»™c cáº¥u hÃ¬nh láº¡i model vá»›i RAG má»›i
                st.session_state.model_configured = False
                # Hiá»ƒn thá»‹ preview
                st.session_state.show_markdown_preview = True
            else:
                st.error(message)
    
    with col2:
        if st.session_state.data_markdown_content:
            st.subheader("Preview file markdown")
            st.info("Hiá»ƒn thá»‹ 500 kÃ½ tá»± Ä‘áº§u tiÃªn. Nháº¥n nÃºt 'Xem Ä‘áº§y Ä‘á»§' Ä‘á»ƒ xem toÃ n bá»™ ná»™i dung.")
            st.code(st.session_state.data_markdown_content[:500] + "..." if len(st.session_state.data_markdown_content) > 500 else st.session_state.data_markdown_content)
            
            if st.button("Xem Ä‘áº§y Ä‘á»§ ná»™i dung markdown"):
                st.session_state.show_markdown_preview = True
    
    # Hiá»ƒn thá»‹ markdown Ä‘áº§y Ä‘á»§ trong modal
    if st.session_state.show_markdown_preview and st.session_state.data_markdown_content:
        with st.expander("Ná»™i dung Ä‘áº§y Ä‘á»§ cá»§a file markdown", expanded=True):
            st.markdown(st.session_state.data_markdown_content)
            if st.button("ÄÃ³ng preview"):
                st.session_state.show_markdown_preview = False

# HÃ m hiá»ƒn thá»‹ tab Chat
def display_chat_tab():
    # Hiá»ƒn thá»‹ lá»‹ch sá»­ tin nháº¯n
    col1, col2 = st.columns([5, 1])
    with col1:
        st.subheader("Lá»‹ch sá»­ há»™i thoáº¡i")
    with col2:
        # NÃºt xÃ³a lá»‹ch sá»­ Ä‘áº·t á»Ÿ Ä‘Ã¢y, bÃªn cáº¡nh tiÃªu Ä‘á» lá»‹ch sá»­ há»™i thoáº¡i
        if st.button("XÃ³a lá»‹ch sá»­", type="primary"):
            st.session_state.messages = []
            st.success("ÄÃ£ xÃ³a lá»‹ch sá»­ há»™i thoáº¡i!")
            st.experimental_rerun()

    chat_container = st.container(height=500)

    with chat_container:
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                with st.chat_message("user"):
                    st.write(f"**{msg['timestamp']}**")
                    st.markdown(msg["content"])
            else:
                with st.chat_message("assistant"):
                    st.write(f"**{msg['timestamp']}**")
                    st.markdown(msg["content"])

# Giao diá»‡n chÃ­nh
st.title("ğŸ’¬ Gemini Chat Demo" + (" vá»›i RAG" if st.session_state.use_rag else ""))

# Táº¡o tabs cho Ä‘iá»u hÆ°á»›ng giá»¯a cÃ¡c chá»©c nÄƒng
tab_chat, tab_rag, tab_settings = st.tabs(["Chat", "CÃ i Ä‘áº·t RAG", "CÃ i Ä‘áº·t Model"])

# Hiá»ƒn thá»‹ ná»™i dung theo tab Ä‘Æ°á»£c chá»n
with tab_chat:
    display_chat_tab()
    
with tab_rag:
    display_rag_tab()
    
with tab_settings:
    display_settings_tab()

# Äáº·t chat_input á»Ÿ má»©c top-level, bÃªn ngoÃ i tabs
st.subheader("Nháº­p cÃ¢u há»i")
user_input = st.chat_input("Nháº­p tin nháº¯n cá»§a báº¡n...")

# Xá»­ lÃ½ input cá»§a ngÆ°á»i dÃ¹ng
if user_input:
    if not st.session_state.model_configured:
        # Tá»± Ä‘á»™ng cáº¥u hÃ¬nh model náº¿u chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh
        with st.spinner("Äang cáº¥u hÃ¬nh model..."):
            if configure_gemini_model():
                send_message(user_input)
                st.experimental_rerun()
            else:
                st.error("CÃ³ lá»—i khi cáº¥u hÃ¬nh model. Vui lÃ²ng kiá»ƒm tra láº¡i cÃ i Ä‘áº·t.")
    else:
        send_message(user_input)
        st.experimental_rerun()

# Hiá»ƒn thá»‹ thÃ´ng tin vá» á»©ng dá»¥ng
st.sidebar.markdown("---")
st.sidebar.caption("Gemini Chat Demo vá»›i RAG")
st.sidebar.caption("Táº¡o bá»Ÿi Streamlit vÃ  Gemini API") 