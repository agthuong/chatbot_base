import streamlit as st
import google.generativeai as genai
from datetime import datetime
import os
from dotenv import load_dotenv

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env n·∫øu c√≥
load_dotenv()

# C·∫•u h√¨nh trang Streamlit
st.set_page_config(
    page_title="Gemini Chat Demo v·ªõi RAG",
    page_icon="üí¨",
    layout="wide"
)

# API key c·ªë ƒë·ªãnh
GEMINI_API_KEY = "AIzaSyA0BgHLDCU9yoiv7JAbCoUmJrrtzkkWoV4"

# H√†m ƒë·ªçc file data.markdown
def read_markdown_file(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()
    except Exception as e:
        return f"L·ªói khi ƒë·ªçc file: {e}"

# H√†m kh·ªüi t·∫°o session state
def init_session_state():
    # S·ª≠ d·ª•ng API key c·ªë ƒë·ªãnh
    if "api_key" not in st.session_state:
        st.session_state.api_key = GEMINI_API_KEY
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "gemini_model" not in st.session_state:
        st.session_state.gemini_model = "gemini-2.0-flash"
    if "temperature" not in st.session_state:
        st.session_state.temperature = 0.7
    if "max_output_tokens" not in st.session_state:
        st.session_state.max_output_tokens = 2048
    if "top_k" not in st.session_state:
        st.session_state.top_k = 40
    if "top_p" not in st.session_state:
        st.session_state.top_p = 0.95
    if "system_prompt" not in st.session_state:
        st.session_state.system_prompt = "B·∫°n l√† tr·ª£ l√Ω AI h·ªØu √≠ch, l·ªãch s·ª± v√† trung th·ª±c."
    if "full_system_prompt" not in st.session_state:
        st.session_state.full_system_prompt = f"[SYSTEM]: {st.session_state.system_prompt}"
    if "model_configured" not in st.session_state:
        st.session_state.model_configured = False
    if "data_markdown_content" not in st.session_state:
        st.session_state.data_markdown_content = ""
    if "use_rag" not in st.session_state:
        st.session_state.use_rag = True
    if "markdown_file_path" not in st.session_state:
        st.session_state.markdown_file_path = "data.markdown"
    if "active_tab" not in st.session_state:
        st.session_state.active_tab = "Chat"
    if "show_markdown_preview" not in st.session_state:
        st.session_state.show_markdown_preview = False

# Kh·ªüi t·∫°o session state
init_session_state()

# H√†m kh·ªüi t·∫°o model Gemini
def configure_gemini_model():
    try:
        genai.configure(api_key=st.session_state.api_key)
        
        generation_config = {
            "temperature": st.session_state.temperature,
            "max_output_tokens": st.session_state.max_output_tokens,
            "top_k": st.session_state.top_k,
            "top_p": st.session_state.top_p,
        }
        
        # T·∫°o system prompt v√† l∆∞u v√†o session state ƒë·ªÉ s·ª≠ d·ª•ng khi g·ª≠i tin nh·∫Øn
        base_system_prompt = st.session_state.system_prompt
        
        if st.session_state.use_rag and st.session_state.data_markdown_content:
            # N·∫øu s·ª≠ d·ª•ng RAG, th√™m th√¥ng tin t·ª´ file markdown v√†o system prompt
            full_system_prompt = f"""[SYSTEM]: {base_system_prompt}

B·∫°n c√≥ quy·ªÅn truy c·∫≠p v√†o th√¥ng tin sau:

{st.session_state.data_markdown_content}

Khi ng∆∞·ªùi d√πng h·ªèi v·ªÅ n·ªôi dung li√™n quan ƒë·∫øn th√¥ng tin tr√™n, h√£y s·ª≠ d·ª•ng th√¥ng tin n√†y ƒë·ªÉ tr·∫£ l·ªùi. 
N·∫øu c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn th√¥ng tin n√†y, h√£y tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung c·ªßa b·∫°n."""
        else:
            # N·∫øu kh√¥ng s·ª≠ d·ª•ng RAG, s·ª≠ d·ª•ng system prompt g·ªëc
            full_system_prompt = f"[SYSTEM]: {base_system_prompt}"
        
        # L∆∞u system prompt v√†o session state
        st.session_state.full_system_prompt = full_system_prompt
        
        model = genai.GenerativeModel(
            model_name=st.session_state.gemini_model,
            generation_config=generation_config
        )
        
        # T·∫°o chat session nh∆∞ng kh√¥ng s·ª≠ d·ª•ng system_instruction
        chat = model.start_chat(history=[])
        
        st.session_state.model = model
        st.session_state.chat = chat
        st.session_state.model_configured = True
        return True
    except Exception as e:
        st.error(f"L·ªói khi c·∫•u h√¨nh model Gemini: {e}")
        st.session_state.model_configured = False
        return False

# H√†m g·ª≠i tin nh·∫Øn v√† nh·∫≠n ph·∫£n h·ªìi, t√≠ch h·ª£p system prompt
def send_message(user_message):
    if not st.session_state.model_configured:
        st.error("Vui l√≤ng c·∫•u h√¨nh API key v√† c√°c th√¥ng s·ªë tr∆∞·ªõc khi g·ª≠i tin nh·∫Øn.")
        return
    
    # Th√™m tin nh·∫Øn ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠
    timestamp = datetime.now().strftime("%H:%M:%S")
    st.session_state.messages.append({"role": "user", "content": user_message, "timestamp": timestamp})
    
    try:
        # G·ª≠i tin nh·∫Øn k·∫øt h·ª£p system prompt v√† user message
        combined_message = f"{st.session_state.full_system_prompt}\n\n[USER]: {user_message}"
        response = st.session_state.chat.send_message(combined_message)
            
        response_text = response.text
        
        # Th√™m ph·∫£n h·ªìi v√†o l·ªãch s·ª≠
        timestamp = datetime.now().strftime("%H:%M:%S")
        st.session_state.messages.append({"role": "model", "content": response_text, "timestamp": timestamp})
    except Exception as e:
        st.error(f"L·ªói khi g·ªçi API Gemini: {e}")

# H√†m t·∫£i file markdown
def load_markdown_file():
    try:
        file_path = st.session_state.markdown_file_path
        content = read_markdown_file(file_path)
        st.session_state.data_markdown_content = content
        return True, f"ƒê√£ t·∫£i file {file_path} th√†nh c√¥ng."
    except Exception as e:
        return False, f"L·ªói khi t·∫£i file: {e}"

# H√†m hi·ªÉn th·ªã tab C√†i ƒë·∫∑t
def display_settings_tab():
    # Hi·ªÉn th·ªã th√¥ng b√°o API key ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh s·∫µn
    st.success(f"API Key ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh t·ª± ƒë·ªông")
    
    # TƒÉng kh√¥ng gian cho System Prompt b·∫±ng c√°ch s·ª≠ d·ª•ng full width
    st.subheader("System Prompt")
    system_prompt = st.text_area("System Prompt:", value=st.session_state.system_prompt, height=300)
    if system_prompt != st.session_state.system_prompt:
        st.session_state.system_prompt = system_prompt
        st.session_state.model_configured = False
    st.info("L∆∞u √Ω: Thay ƒë·ªïi System Prompt ch·ªâ c√≥ hi·ªáu l·ª±c sau khi nh·∫•n '√Åp d·ª•ng c√†i ƒë·∫∑t'.")
    
    # T·∫°o 2 c·ªôt cho c√†i ƒë·∫∑t model
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("C√†i ƒë·∫∑t Model")
        model_options = ["gemini-2.5-flash-preview-04-17", "gemini-2.5-pro-preview-05-06", "gemini-2.0-flash"]
        selected_model = st.selectbox("Model:", model_options, index=model_options.index(st.session_state.gemini_model) if st.session_state.gemini_model in model_options else 0)
        if selected_model != st.session_state.gemini_model:
            st.session_state.gemini_model = selected_model
            st.session_state.model_configured = False
    
    with col2:
        with st.expander("C√†i ƒë·∫∑t n√¢ng cao", expanded=True):
            temperature = st.slider("Temperature:", min_value=0.0, max_value=1.0, value=st.session_state.temperature, step=0.05)
            if temperature != st.session_state.temperature:
                st.session_state.temperature = temperature
                st.session_state.model_configured = False
            
            max_tokens = st.slider("Max Output Tokens:", min_value=100, max_value=8192, value=st.session_state.max_output_tokens, step=100)
            if max_tokens != st.session_state.max_output_tokens:
                st.session_state.max_output_tokens = max_tokens
                st.session_state.model_configured = False
                
            top_k = st.slider("Top K:", min_value=1, max_value=100, value=st.session_state.top_k, step=1)
            if top_k != st.session_state.top_k:
                st.session_state.top_k = top_k
                st.session_state.model_configured = False
                
            top_p = st.slider("Top P:", min_value=0.0, max_value=1.0, value=st.session_state.top_p, step=0.05)
            if top_p != st.session_state.top_p:
                st.session_state.top_p = top_p
                st.session_state.model_configured = False
    
    # N√∫t c·∫•u h√¨nh model
    if st.button("√Åp d·ª•ng c√†i ƒë·∫∑t", type="primary"):
        with st.spinner("ƒêang c·∫•u h√¨nh model..."):
            if configure_gemini_model():
                st.success("C·∫•u h√¨nh model th√†nh c√¥ng!")

# H√†m hi·ªÉn th·ªã tab RAG
def display_rag_tab():
    st.subheader("C√†i ƒë·∫∑t RAG")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        use_rag = st.checkbox("S·ª≠ d·ª•ng RAG", value=st.session_state.use_rag)
        if use_rag != st.session_state.use_rag:
            st.session_state.use_rag = use_rag
            st.session_state.model_configured = False
        
        markdown_file_path = st.text_input("ƒê∆∞·ªùng d·∫´n file markdown:", value=st.session_state.markdown_file_path)
        if markdown_file_path != st.session_state.markdown_file_path:
            st.session_state.markdown_file_path = markdown_file_path
        
        if st.button("T·∫£i file markdown", type="primary"):
            success, message = load_markdown_file()
            if success:
                st.success(message)
                # Reset model_configured ƒë·ªÉ bu·ªôc c·∫•u h√¨nh l·∫°i model v·ªõi RAG m·ªõi
                st.session_state.model_configured = False
                # Hi·ªÉn th·ªã preview
                st.session_state.show_markdown_preview = True
            else:
                st.error(message)
    
    with col2:
        if st.session_state.data_markdown_content:
            st.subheader("Preview file markdown")
            st.info("Hi·ªÉn th·ªã 500 k√Ω t·ª± ƒë·∫ßu ti√™n. Nh·∫•n n√∫t 'Xem ƒë·∫ßy ƒë·ªß' ƒë·ªÉ xem to√†n b·ªô n·ªôi dung.")
            st.code(st.session_state.data_markdown_content[:500] + "..." if len(st.session_state.data_markdown_content) > 500 else st.session_state.data_markdown_content)
            
            if st.button("Xem ƒë·∫ßy ƒë·ªß n·ªôi dung markdown"):
                st.session_state.show_markdown_preview = True
    
    # Hi·ªÉn th·ªã markdown ƒë·∫ßy ƒë·ªß trong modal
    if st.session_state.show_markdown_preview and st.session_state.data_markdown_content:
        with st.expander("N·ªôi dung ƒë·∫ßy ƒë·ªß c·ªßa file markdown", expanded=True):
            st.markdown(st.session_state.data_markdown_content)
            if st.button("ƒê√≥ng preview"):
                st.session_state.show_markdown_preview = False

# H√†m hi·ªÉn th·ªã tab Chat
def display_chat_tab():
    # Hi·ªÉn th·ªã l·ªãch s·ª≠ tin nh·∫Øn
    col1, col2 = st.columns([5, 1])
    with col1:
        st.subheader("L·ªãch s·ª≠ h·ªôi tho·∫°i")
    with col2:
        # N√∫t x√≥a l·ªãch s·ª≠ ƒë·∫∑t ·ªü ƒë√¢y, b√™n c·∫°nh ti√™u ƒë·ªÅ l·ªãch s·ª≠ h·ªôi tho·∫°i
        if st.button("X√≥a l·ªãch s·ª≠", type="primary"):
            st.session_state.messages = []
            st.success("ƒê√£ x√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i!")
            st.experimental_rerun()

    chat_container = st.container(height=500)

    with chat_container:
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                with st.chat_message("user"):
                    st.write(f"**{msg['timestamp']}**")
                    st.markdown(msg["content"])
            else:
                with st.chat_message("assistant"):
                    st.write(f"**{msg['timestamp']}**")
                    st.markdown(msg["content"])

# Giao di·ªán ch√≠nh
st.title("üí¨ Gemini Chat Demo" + (" v·ªõi RAG" if st.session_state.use_rag else ""))

# T·∫°o tabs cho ƒëi·ªÅu h∆∞·ªõng gi·ªØa c√°c ch·ª©c nƒÉng
tab_chat, tab_rag, tab_settings = st.tabs(["Chat", "C√†i ƒë·∫∑t RAG", "C√†i ƒë·∫∑t Model"])

# Hi·ªÉn th·ªã n·ªôi dung theo tab ƒë∆∞·ª£c ch·ªçn
with tab_chat:
    display_chat_tab()
    
with tab_rag:
    display_rag_tab()
    
with tab_settings:
    display_settings_tab()

# ƒê·∫∑t chat_input ·ªü m·ª©c top-level, b√™n ngo√†i tabs
st.subheader("Nh·∫≠p c√¢u h·ªèi")
user_input = st.chat_input("Nh·∫≠p tin nh·∫Øn c·ªßa b·∫°n...")

# X·ª≠ l√Ω input c·ªßa ng∆∞·ªùi d√πng
if user_input:
    if not st.session_state.model_configured:
        # T·ª± ƒë·ªông c·∫•u h√¨nh model n·∫øu ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh
        with st.spinner("ƒêang c·∫•u h√¨nh model..."):
            if configure_gemini_model():
                send_message(user_input)
                st.experimental_rerun()
            else:
                st.error("C√≥ l·ªói khi c·∫•u h√¨nh model. Vui l√≤ng ki·ªÉm tra l·∫°i c√†i ƒë·∫∑t.")
    else:
        send_message(user_input)
        st.experimental_rerun()

# Hi·ªÉn th·ªã th√¥ng tin v·ªÅ ·ª©ng d·ª•ng
st.sidebar.markdown("---")
st.sidebar.caption("Gemini Chat Demo v·ªõi RAG")
st.sidebar.caption("T·∫°o b·ªüi Streamlit v√† Gemini API") 